{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 11 // More neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we build a recommender system for the full \"small\" MovieLens dataset. Previously we saw how to use matrix decomposition to represent each movie and each user as a vector of latent variables. Here we use neural networks to learn the \"weights\" in these latent factors. \n",
    "\n",
    "Load the required packages and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(keras)\n",
    "\n",
    "load(\"data/movielens-small.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An \"embedding\" is a function mapping discrete units (for example, words, users, or movies) to high-dimensional vectors (perhaps 200 to 500 dimensions). Each user, for example, is represented by a vector. The embedding function can be thought of as a lookup table, parameterized by a matrix, with a row for each discrete unit.\n",
    "\n",
    "In this case we have two embeddings, one for users and one for movies. To use embeddings in keras, you must first transformed the set of discrete units (e.g. movie ids) so that they are contiguous integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings <- ratings %>% mutate(userId = -1 + as.numeric(factor(userId)),\n",
    "                              movieId = -1 + as.numeric(factor(movieId)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the number of users and movies in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users <- length(unique(ratings$userId))\n",
    "n_movies <- length(unique(ratings$movieId))\n",
    "n_users\n",
    "n_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And choose the number of dimensions to use in each embedding (i.e. the number of latent factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_factors <- 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly assign 80% of the ratings to the training data and keep the remaining 20% aside as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_indicator <- (runif(nrow(ratings)) < 0.8)\n",
    "training_ratings <- ratings[train_indicator,]\n",
    "test_ratings <- ratings[-train_indicator,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build up the model, using the Keras functional API. The way you build a functional model is quite different to how one builds up the sequential model, and will take a bit of practice to get used to. The main features are:\n",
    "\n",
    "* A layer instance is callable (on a tensor), and it returns a tensor\n",
    "* Input tensor(s) and output tensor(s) can then be used to define a Model\n",
    "\n",
    "For example, below we specify the shape of our input layers for user and movie embeddings. These are just a single value, representing the index of the user or movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_in <- layer_input(shape = c(1), dtype = 'int64', name = 'user_in')\n",
    "movie_in <- layer_input(shape = c(1), dtype = 'int64', name = 'movie_in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the embedding by calling the layer instance (`layer_embedding`) on the input tensor `user_in`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_emb <- user_in %>% layer_embedding(input_dim = n_users, output_dim = n_factors, input_length = 1)\n",
    "movie_emb <- movie_in %>% layer_embedding(input_dim = n_movies, output_dim = n_factors, input_length = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as writing\n",
    "```\n",
    "user_emb <- layer_embedding(input_dim = n_users, output_dim = n_factors, input_length = 1)(user_in)\n",
    "movie_emb <- layer_embedding(input_dim = n_movies, output_dim = n_factors, input_length = 1)(movie_in)\n",
    "```\n",
    "which is the way you will probably see this done in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define how we get our output tensor. This is by taking the embedding layer (i.e. the transformed inputs) and adding some further layers. In this case, we add a single dense hidden layer of 128 neurons, and then connect these up to a single output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions <- layer_concatenate(c(user_emb, movie_emb)) %>%\n",
    "  layer_flatten() %>% \n",
    "  layer_dropout(0.3) %>%\n",
    "  layer_dense(70, activation='relu') %>% \n",
    "  layer_dropout(0.75) %>%\n",
    "  layer_dense(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get to the second step in the functional model: input and output tensors are can then be used to define a `keras_model`. Note that we have **two** input tensors, one for users and one for movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model <- keras_model(c(user_in, movie_in), predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compile the model, fit and evaluate te model in much the same way as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model %>% compile(optimizer='Adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model %>% fit(list(training_ratings$userId, training_ratings$movieId), \n",
    "           training_ratings$rating, \n",
    "           batch_size=64, \n",
    "           epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% evaluate(list(test_ratings$userId, test_ratings$movieId), \n",
    "                test_ratings$rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting tweet classification\n",
    "\n",
    "Previously we used a simple bag-of-words model to predict whether a tweet made by Donald Trump was made before or after he became president. Here we use a CNN to do the same thing. The CNN allows us to exploit some of the relationships that exist between words. \n",
    "\n",
    "Load the data and required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(stringr)\n",
    "library(lubridate)\n",
    "library(tidytext)\n",
    "\n",
    "load(\"data/trump-tweets.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process the data, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets <- as.tibble(tweets)\n",
    "# parse the date and add some date related variables\n",
    "tweets <- tweets %>% \n",
    "  mutate(text = str_replace_all(text, \"@realdonaldtrump\", \"\")) %>%\n",
    "  mutate(date = parse_datetime(str_sub(tweets$created_at,5,30), \"%b %d %H:%M:%S %z %Y\")) %>% \n",
    "  mutate(is_prez = (date > ymd(20161108))) %>%\n",
    "  mutate(month = make_date(year(date),month(date)))\n",
    "\n",
    "# take a sample of 1000 tweets before and after he became president\n",
    "tweets <- tweets %>% group_by(is_prez) %>% \n",
    "  sample_n(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features <- 200        # choose max_features most popular words\n",
    "minlen <- 5                # exclude tweets shorter than this\n",
    "maxlen <- 32               # longest tweet (for padding)\n",
    "embedding_dims <- 10       # number of dimensions for word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use Keras to tokenize the tweets - this turns each tweet into a vector of integers, each integer representing a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text_tokenizer(num_words = max_features)\n",
    "fit_text_tokenizer(tokenizer, tweets$text)\n",
    "sequences = tokenizer$texts_to_sequences(tweets$text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove tweets with just a single word, or we get an error. I also throw out very short tweets (less than 5 words), but this is not strictly needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_ok <- unlist(lapply(sequences, length)) > minlen\n",
    "# outcome variable (1 = when president, 0 = before)\n",
    "y <- as.integer(tweets$is_prez[seq_ok])\n",
    "\n",
    "# exclude short sequences\n",
    "lengthIs <- function(n) function(x) length(x)>n\n",
    "sequences <- Filter(lengthIs(minlen), sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split up the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test <- list()\n",
    "train <- list()\n",
    "train_id <- sample(1:length(sequences),\n",
    "                size = 0.9*length(sequences), \n",
    "                replace=F)\n",
    "test$x <-  sequences[-train_id]\n",
    "train$x <- sequences[train_id]\n",
    "\n",
    "train$y <- y[train_id]\n",
    "test$y <-  y[-train_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequences are of different length. We \"pad\" the shorter sequences with zeros so that all padded sequences are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train <- train$x %>%\n",
    "  pad_sequences(maxlen = maxlen)\n",
    "\n",
    "x_test <- test$x %>%\n",
    "  pad_sequences(maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model <- keras_model_sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model %>% \n",
    "  # embedding layer maps vocab indices into embedding_dims dimensions\n",
    "  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%\n",
    "  layer_dropout(0.2) %>%\n",
    "  # convolutional layer\n",
    "  layer_conv_1d(\n",
    "    filters = 250, kernel_size = 3, \n",
    "    padding = \"valid\", activation = \"relu\", strides = 1\n",
    "  ) %>%\n",
    "  # max pooling layer\n",
    "  layer_global_max_pooling_1d() %>%\n",
    "  # dense hidden layer\n",
    "  layer_dense(128) %>%\n",
    "  layer_dropout(0.2) %>%\n",
    "  layer_activation(\"relu\") %>%\n",
    "  # single unit output layer\n",
    "  layer_dense(1) %>%\n",
    "  layer_activation(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "  loss = \"binary_crossentropy\",\n",
    "  optimizer = \"adam\",\n",
    "  metrics = \"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model %>%\n",
    "  fit(\n",
    "    x_train, train$y,\n",
    "    batch_size = 32,\n",
    "    epochs = 10,\n",
    "    validation_data = list(x_test, test$y)\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
